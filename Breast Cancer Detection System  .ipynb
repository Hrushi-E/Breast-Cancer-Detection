{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a breast cancer detection system\n",
    "\n",
    "In this Notebook, you will build your own breat cancer detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification has different algorithms, which we will study in detail but for a couple of videos, we will be diving into the one of the most powerful classification algorithms, **Logistic Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a classification algorithms, which will tell you the probability that an instance belong to this class or not with some threshold. If our probability is greater than 0.5 or 50 % then we will say this belongs to the class, means positive_label **1** ot if lesser we denote negative class or vice versa **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example \n",
    "\n",
    "- What is the probability that this image is cat image? \n",
    "\n",
    "If our probability is above 0.5 which is our threshold, then it is a cat image **1** otherwise it is non cat **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fact about Logistic Regression\n",
    "\n",
    "It is called as regression because it's underlying technique is quite the same as Linear Regression. The term “Logistic” is taken from the Logit function that is used in this method of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start diving in logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Function Or Prediction Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does the same work as linear regression does, logistic regression computes the features weights and the bias term and multiply with the respected features and sum them up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Y} = h(x) =  \\Theta_0*x_{0} + \\Theta_1*x_{1} + \\Theta_2*x_{2} \\ + \\Theta_3*x_{3} + \\Theta_n*x_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorized form of above equation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Y} = h(x) = (x^T\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But one thing more it does, it computes the sigmoid of the hypothesis, which will enable the hypothesis to give output between 0 and 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{Y} = h(x) =  \\sigma(x^T\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote $x^T\\Theta$ as z, so we can write our function:- \n",
    "$\\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma(z)$ is a sigmoid function which outputs the number between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid Function Can be Defined as:-**\n",
    "$\\sigma(z) = \\frac{1}{1+exp(-z)}$ \n",
    "\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$ \n",
    "\n",
    "\n",
    "It forms S shaped figure."
   ]
  },
  {
   "attachments": {
    "index.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEjCAYAAAAomJYLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUZfbA8e9JpROaCESKUqQIKCqKVAEBXQVFFFQElHXxp7CuCi66tpXdRV07NhYQXFkBURGxABZERZpIkyIoCIEgNZAAIe38/riXOIRJMjNkcjPkfJ5nHubOvee+5w6Teee294iqYowxxuQV5XUCxhhjSibrIIwxxvhlHYQxxhi/rIMwxhjjl3UQxhhj/LIOwhhjjF/WQZhSSURuFpF5Ja1dEVkgIkOLMydj8mMdhDmtiUh7EVkkIgdFZL+IfCsiF6nqVFW9orjz8apdY0IR43UCxoSLiFQC5gB3AjOAOKADcMzLvIyJFLYHYU5njQFU9W1VzVbVo6o6T1VXi8hgEfnm+IIicoWIbHT3NF4Rka+OH+pxl/1WRJ4TkRQR+UVE2rmvbxeR3SIyyGddlUXkTRHZIyK/isjfRCTKZ12+7XYXkQ1uu+MAKbZ3x5hCWAdhTmc/AdkiMkVEeolIFX8LiUh1YCYwGqgGbATa5VmsLbDanf8/YBpwEdAQuAUYJyIV3GVfAioDZwOdgFuBIfm0+y7wN6A68DNwWagba0xRsw7CnLZU9RDQHlDgP8AeEZktIjXzLHol8KOqvqeqWcCLwK48y2xR1TdUNRuYDpwF/F1Vj6nqPCADaCgi0cCNwGhVTVXVrcAzwEA/KV4JrFPVmaqaCTzvp11jPGMdhDmtqep6VR2sqolAC6A2zhexr9rAdp8YBZLyLPObz/Oj7nJ5X6uAsycQB/zqM+9XoI6f9Py1u93PcsZ4wjoIU2qo6gZgMk5H4SsZSDw+ISLiOx2kvUAmUM/ntbrADj/LJuPsifi2e5af5YzxhHUQ5rQlIueKyH0ikuhOnwUMABbnWfQj4DwR6SMiMcBdwJmhtOkegpoB/ENEKopIPeBe4C0/i38ENBeR69x2R4TarjHhYB2EOZ2l4pxcXiIih3E6hrXAfb4LqepeoB/wFLAPaAYsJ/TLYYcDh4FfgG9wTmpPyruQT7tj3XYbAd+G2KYxRU6sYJAxJ3IvSU0CblbVL73Oxxiv2B6EMYCI9BCRBBGJBx7EuR8h76EoY0oV6yCMcVyKcx/CXuBqoI+qHvU2JWO8ZR2EKdGKa1A9VX1MVaupakVVbYtzT0PEDqonIj+KSGev8zCRzToI47n8BtQD7wa3O5V2ReQxEckUkTSfx6iiztGnvckiMsb3NVVtrqoLwtWmKR1ssD7jqdN4QL3pqnqL10kYcypsD8J4Ld8B9cDv4HYRO6ieu2fxls90fRFR9x6I44etnnC3IVVE5rnjNR1f/vieVoq7PYNF5A7gZmCUu6fyobvsVhHp5j6PF5HnRWSn+3jePRmPiHQWkST3fpHdIpIsIieNG2VKJ+sgjNcCGlAPSs2geje5OZyBszd1v5tDXeATN+caQGtgpaqOB6YCT6lqBVW92s86HwIucWNaARe723LcmTjvQx3gduDlgv4fTOlhHYTxVBAD6kFkDap3g/tL//ijduHvBgBvqOpP7hVUM3C+1MHZS/jM3dPKVNV9qroywHXejPMe7FbVPcDjnLidme78TFX9GEgDmgS4bnMasw7CeC7AAfUgsgbVm6GqCT6PnYUsf5xvx3PEzRWcju7nANeRV21O3k7fDmuf2+H6a9eUYtZBmBKlgAH1IPIH1TsMlPOZDmbcpe3AOfnMK2w4hJ2cvJ2BdlimFLMOwngqiAH1IPIH1VsJdBSRuiJSGedcSqCmAt1E5AYRiRGRaiJy/PDTbzjnUfLzNvA3Eanhnk95BP/bacwJrIMwXgtoQD2I/EH1VHU+znmR1cD3OJf3Bhq7DedcyH3AfpzOppU7eyLQzD3XMctP+Bic92k1sAZY4b5mTIFssD4TscQG1TMmrGwPwkQUsUH1jCk21kGYSGOD6hlTTOwQkzHGGL9sD8IYY4xfETdYX0JCgjZs2DCk2MOHD1O+fPmQ2z6V+EiM9bLtSIz1su1IjPWy7dK4zd9///1eVa0RVJCqRtSjcePGGqovv/wy5NhTjY/EWC/bjsRYL9uOxFgv2y6N2wws1yC/b+0QkzHGGL+sgzDGGOOXdRDGGGP8sg7CGGOMX9ZBGGOM8StsHYSITHJLGK7NZ76IyIsisllEVovIBeHKxRhjTPDCuQcxGehZwPxeOKNiNgLuAF4NYy7GGGOCFLYb5VR1oYjUL2CR3sCb7vW5i90B2GqpanK4cjLGnB6ysuDo0Wj27YP0dMjIgGPHICM9h8yjWWQcySIzPZus9CyyjmU7j/QssjOyyc7IZtPGTHYu+InszGxyMnPIzsgmJ8t5npN1/JGNZuWQk5lNTg6oOpWZ9uzZz7JqC51p9wGBPz9w4CBfVVmY+9rvT/JMquadRa0me+jcuYjfzAKEdSwmt4OYo6onVQcTkTnAWFX9xp3+HHhAVZf7WfYOnL0MatSo0WbGjBkh5ZOWlkaFCqFXUjyV+EiM9bLtSIz1su1Iis3JgZSUWA4ciGPXriyOHavMwYMxpKXGcGR3Jsd+O0r6viyOHlQOp8dyJLssh3PKciSnLOk58RzRsmQRG1K+kUjIyX1+R5NZ9H+takjr6dKly/eqemEwMV4OtSF+XvPbW6nqeGA8QJMmTbRziF3oggULCDX2VOMjMdbLtiMx1su2S1KsKuzYARs3wk8/wZYt8OuvzmPHDti1y9kD8Kcch0kghcocpFLcMSqVy6ZO3F4qxGZQLjaTcrGZlI3LpmxcFhnpKZxRvSJxsUp8vBIfB3FxEBcvxMYLMbFRxMY7j+jYKGLio4mOjyE6Lpqft/5M0xZNc6ej4mOJioshOj6GqLgYJM6Z9n0uKBIlfPfdIi5rfxkiIFGS+y+4077PBZATl1n49UI6ue/Z8decBfP79/czAQsWVD2lz2ewvOwgkjixrm8iVifXmIiSnS2sWAFLlsCqVc5jzRo4fPj3ZeJic6hb7TD1yu6mm2yldvkN1Dq4njPZRQ32UKPcEao1P5MqresRd35zaNkSWrSAypULbPtUOrZ9C6Jp0rlDSLHxP5elQu1KIcUCEBeLxEXGHpCXHcRs4G4RmYZTcvKgnX8wpmTLyoJly+Czz+CLL2Dx4vakpzvzqiTk0LLeQW674FeaZq2hyb5FNPr1M+oc+5moXQpRUdC4MVzaEs47D1peweIjR2h2442//2I2JUrYOggReRvoDFQXkSTgUXAOHKrqa8DHODV2NwNHgCHhysUYE7qjR+HTT+Hdd2HOHDh4EESU8+vtZ8BZS7mi7EouSX6fenuWISluUPXq0KoVXHW1s0dw3nnQrBmULXvCutMXLLDOoQQL51VMAwqZr8Bd4WrfGBM6VfhukfKfF47wzofxHE6PoVpcKteV+4xeUdPpkvMZ1bfuIyc2lqjmzaFXS2h54++dQc2a9sV/Goi4ehDGmPA5dgze/PtWnn8e1h2pTwVyGMAb9GcanWpuIaZlM6cTaDkOWrbk65076dStm9dpmzCxDsIYw+HD8Prr8MzT2ezcVZ8LYlczoeOb3Ng7nQoXNYXz3oWEhJPidPduD7I1xcU6CGNKsZwcmD+/Jrfc4lyC2qXSD0wp8xhdl45FzrvV6/SMx2ywPmNKqe+/h3bt4J//bEqtWsrCq57ki0MX0W3qbch5J93bakoh6yCMKWUyM+Gxx6BtW+fmtQceWM+Swa/R4aO/wkMPwXXXeZ2iKSGsgzCmFNmwAS69FB5/HAYMgHXr4Mba84m6ZwRcdZUzwxiXnYMwppT48EO4+WaIj3fuabjuOiApifKPPQYNGsBbb0F0tNdpmhLE9iCMOc2pwr/+Bb17Ozcy//CD2zmkp8N11xGVng6zZvm9SsmUbtZBGHMay8qCwYPhwQehf3/4+mtITMTpNe68E5YtY8ODDzp3ORuTh3UQxpymMjPhppvgzTedUwtTp/qMdPHyyzB5MjzyCHvbt/cyTVOCWQdhzGno2DHo1w/eeQeeeQYeecRn5IuvvoK//AWuvhoefdTTPE3JFtYOQkR6ishGt+70X/3MryIi77s1qZeKiF18bcwpyspyOocPPoBx4+Dee31mbtvmzDznHOekdJT9RjT5C9unQ0SigZdxak83AwaISN4DnQ8CK1W1JXAr8EK48jGmNFCF4cOdK5Zefhnu8h0O8+hR5+z08ZPSlU6hpoEpFcL58+FiYLOq/qKqGcA0nDrUvpoBnwOo6gagvojUDGNOxpzWnnoKXnsN/vpX+L//85mhCsOGObdPT50K557rWY4mcoStJrWIXA/0VNWh7vRAoK2q3u2zzD+BMqp6r4hcDCxyl/k+z7qsJrXVWC7xsV62nZaWxtKlZ/PEE824/PLfeOih9SccPaozcyaNXn6ZLYMH8+ugQSUiZy/bLo3bHEpNalQ1LA+gHzDBZ3og8FKeZSoBbwArgf8Cy4BWBa23cePGGqovv/wy5NhTjY/EWC/bjsRYL9ueNGmplimj2qGDanp6nplffKEaHa3ap49qdnaRthup71dp3GZguQb5PR7OO6kLrTmtqodwK8mJiABb3IcxJkCHD8PjjzejcmXnqqX4eJ+Zv/4KN9zg3CE3ZYqdlDZBCWcHsQxoJCINgB1Af+Am3wVEJAE4os45iqHAQrfTMMYE6K67YNu2csyf7xRyy3XkCFx7rXNDhJ2UNiEIZ8nRLBG5G5gLRAOTVPVHERnmzn8NaAq8KSLZwDrg9nDlY8zpaMoU5zFw4K907Vr/9xmqcMcdsHKlc0lT48ae5WgiV1gH61PVj4GP87z2ms/z74BG4czBmNPVr786ew8dO8KgQb8C9X+f+dxzztVKY8Y4o7SasLnzzjuZPXs2O3fuPH5u9bRhBySNiUDHh1ICZyiN6GifL6bPP4eRI517Hh580JsES5EBAwawYsUKr9MICxvu25gING0afPIJPP881KsHW45f2rFlC9x4IzRt6oy1lDu+hgmXjh07ep1C2NgehDERZu9eGDECLr4Y7r7bZ8bxk9LZ2c5J6YoVPcsxP6pKq1atmDJlSlBxd911F7ffbqcoi5t1EMZEmPvug5QUmDDBp76PKtx+O6xeDW+/DQ0beppjfmbMmMGBAwe46aabCl/Yx8iRI5k6dSqbN28OU2bGH+sgjIkgixY55xweeADOO+/318+aPt057vTPf0LPnt4lWIgXX3yRgQMHEhsbG1Rc/fr1ad++Pa+++mqYMjP+WAdhTIRQdfYeatWC0aN9Zsyfz9n/+Y8zSusDD3iW365duxg0aBA1a9YkKioKEcl9tGnThs2bN7No0SKuv/76E+K++uorRIRPPvkk97UtW7ZwxhlnMGLEiNzX+vbty9SpU8nJySm2bSrtrIMwJkK88w4sXuxcuVq+vPviL7/AjTdyuH59mDTJs5PS6enpdOvWjYULF/LUU0/x4Ycf0qFDBwDuuOMORo4cyeeff0758uVp1arVCbGdOnWiS5cuPPHEEwAcPHiQP/zhD1x88cU899xzucu1a9eO3377jTVr1vjNQVXJysoq9FHUhg4dSmJiIgCJiYkMHTq0yNvwil3FZEwEOHbMGaG1ZUvIHWvv8GHo0weAtU88wSWnMIDcqRozZgzbt29n3bp11KlTB4Bzzz2Xhg0b0r59e/r3788dd9xB06ZNifIz3Mfjjz9Ox44dmTdvHs888wyxsbFMmzaN6NyTLNC8eXOio6NZunTpSZ0MwJQpUxgyZEihuRb1vQoTJkwo0vWVJNZBGBMBxo1zrmCdN889Ma0Kt90GP/4In3xCelycp/lNnTqVP/7xj7mdA8DZZ59NVFQUKSkpgHMIqnr16n7jO3ToQLdu3bj22mtJSEhgyZIlJ41aGhMTQ0JCArt27fK7jquvvpply5YV0RYZsA7CmBLvwAHnsFLPntC9u/viU0/BjBnOv1dcAQsWeJbfhg0b2Lp1K926dTvh9T179pCTk0OtWrUA5zBUuXLl8l1Pw4YN+eyzz3jhhRdyD9nkFR8fT3p6ut95VatWpXLlyiFuhfHH65KjlUXkQxFZJSI/ikjh+4fGlDIvvuhc1vqvf7kvfPqpc5a6f3+4/35PcwNISkoC4Iwzzjjh9blz5xIbG0t3t1erWrVq7t5EXuPHj2fSpEm0atWqwEM2KSkpVK1a1e+8KVOmEBsbW+gjL9+T6YE8unTpEnRMqPFeC9sehE/J0e44Q38vE5HZqrrOZ7G7gHWqerWI1AA2ishUd3RXY0q9Q4ecu6V794bWrYHNm2HAAOdkxIQJJeJO6YSEBAA2btzIBRdcADh7C2PGjOHGG2/M/VXfpEkTvvvuu5Pi58+fz913382ECRNo3Lgxl156KZ988gm9evU6Ybk9e/Zw5MgRGucz8GCoh5iCPSexYMECOnfuHHQ7RRVfnMJ5iCm35CiAiBwvOerbQShQ0a0FUQHYDxT9ZQbGRKhx45y9h4cfBlJTnZPSUVHw/vs+lzJ5q3Xr1px99tk88MADxMTEICI8+eSTpKen8+KLL+Yud9lll/H3v/+dPXv2UKNGDcC5nPUvf/kLo0aN4tZbbwWgW7duPProoyd1EMuXL0dEaNeund88qlWrRrVq1ULaBq8H3EtJSWHIkCG8+eabDBs2jOeee+6kPTIvhPMQUx1gu890kvuar3E4Q37vBNYAf1ZVu8jZGCAtDZ59Fnr1gjYXKAwZAuvXw/Tp0KCB1+nliomJYfbs2dSrV4+BAwdy55130qJFCxYvXkyVKlVyl+vcuTNVq1bl008/BWD37t08+OCDdO/ePfcSV4CHH36YZcuW8dFHH53QzqeffkqnTp1C7gQKEuyAe/Xr1w+pnQ0bNjBq1Ch69uyZ+5g/fz4JCQl06NCBvn37Mnbs2BLROUB4a1L3A3roiTWpL1bV4T7LXA9cBtwLnAPMxyk5eijPuqwmdSmrnxuJsUXd9vTpZ/Haa+cwbtwKev3wCmdPnMjmO+8k6YYbwtpuOGNfeuklduzYwdixY4OKz87Ozr1U9vg5jXDk3aVLF7788stCY4cOHcq0adPyXWb//v28/PLLJCcnc+TIEYYMGUKnTp3ybTsjI4OnnnqKw4cPM2bMmBMu7w0k70CUtJrUlwJzfaZHA6PzLPMR0MFn+gucTsRqUpeAWC/bjsTYomz7yBHVmjVVu3VT1TlzVEVUb75ZNScnrO2GO3b79u1arlw53bhxY1Dxb7/9tjZs2FAzMzNDbttXfrHOV2LhsfXq1ct3flZWlnbt2lVXrFihqqq//fab1qlTJ9+2MzMzdfDgwbpu3Tp955139JVXXgk670BQwmpSF1pyFNgGdAW+FpGaQBPglzDmZExEeOst+O03ePvpJLj5ZucM9fjxJeKk9KlITExk4sSJJCcn53uy2R9VZeLEicTEeHdl/jXXXMO2bdtIS0tj586dtG7dGoBLLrmE117LrYPGxx9/zKpVq064aa+gy3tjYmJ44403AGjatGmYsg+N1yVHnwAmi8gaQIAHVHVvuHIyJhKowgsvQOuW2XT+5xUQG+uclC7gSyaS9O/fP+iYAQMGhCGT4MyePRtwrkIaPHgwK1eu9Lvc6tWrGTVqFCNHjizO9MLC65KjO4ErwpmDMZHm88+dG6TfOP9lZNNPMH++UxXIRITatWvz1ltv8ec//5m4uDiSk5OJioqiZs2aXqcWNBusz5gS5vnn4YzyafT/YRQ88wx06eJ1Sqe9ohxw75ZbbiExMZFmzZrRunVrbrnllqJKs9jZUBvGlCCbNsFHH8EjPEOZW290SseZsAt2wL2tW7fmOy82NjboinkllXUQxpQgLz5+gFjKc2fLRfDarIg/KW0imx1iMqaEOLo3k8lvx9E/fhZnzpkAZct6nZIp5ayDMKaE+O61NNJyyjPi33XhrLO8TscY6yCMKQlU4f1FzTg/7kcuvKut1+kYA1gHYUyJsHzeftYebcIfu2y28w6mxLCT1MaUAP95YhfliOemh8/xOhVjctkehDEeS0uDtxfX57ryH1G5XXOv0zEml3UQxnhs+usppGWXo0/Hn+zwkilRrIMwxmP/eSmdpqwjsZ//OszGeMXrmtQjRWSl+1grItki4r/grDGnobVrYcmvZ/LHM+dwtEF9r9Mx5gRh6yB8alL3ApoBA0Skme8yqvq0qrZW1dY49SK+UtX94crJmJJmyrhUYshk4GD/BWKM8VI49yBya1KragZwvCZ1fgYAb4cxH2NKlOxs+N/bwpV8TPUhV3udjjEn8bomNQAiUg7oCbwbxnyMKVG+/BJ2HqrALfW/gSCK5xhTXDytSe2z7I3ALarq92eU1aQ+PWosn+6xwcY//Wg9vllYle+GPMjuW/tF5Dbb5yty2o64mtQ+894HbgpkvVaTuvhivWw7EmODiT98WLVC3DG9nf+o/vzzKbcdibFetl0at5kQalKH8xBTbk1qEYnDqUk9O+9CIlIZ6AR8EMZcjClRPvgA0jLiuKXJcjj7bK/TMcYvr2tSA1wLzFPVw+HKxZiS5r+vH+Ys9tHxtoZep2JMvjytSe1OTwYmhzMPY0qS336DeV+XZSRTiep/s9fpGJMvu5PamGI2YwZk50Rxc6sfoW5dr9MxJl82mqsxxWz65CO04GdaDLnI61SMKZDtQRhTjJKS4NsV5biRGXD99V6nY0yBrIMwphjNnOn826/NL1DH732jxpQYdojJmGI0Y/IRWvETTYa08zoVYwplexDGFJNt2+C7VeW4UezwkokM1kEYU0zemeEMa3ND221Qs6bH2RhTODvEZEwxmTHlCG1YzzmDO3idijEBsT0IY4rBli2wdG15bpCZ0Lev1+kYExDrIIwpBscPL/W7bCdUr+5xNsYExg4xGVMM3nvrMG3YQIMhnb1OxZiAeVqT2l2ms1uT+kcR+Sqc+RjjhaQkWLK2AtdFzYI+fbxOx5iAhW0PwqcmdXecanLLRGS2qq7zWSYBeAXoqarbROSMcOVjjFdmva+A0Lf9bqha1et0jAmY1zWpbwLeU9VtAKq6O4z5GOOJ9yYfohk/0uT29l6nYkxQwlly9HqcPQPfkqNtVfVun2WeB2KB5kBF4AVVfdPPuqzkaCkrjxiJsf7iDx6M5bo+lzI6aiw9PmhJdgHrjsRtts9X5LRd0kqO9gMm+EwPBF7Ks8w4YDFQHqgObAIaF7ReKzlafLFeth2Jsf7iJ4zPVlBd0emesLYdibFetl0at5kQSo6G8yqmJOAsn+lEYKefZfaqU03usIgsBFoBP4UxL2OKzXuTUmhACq3/aEN7m8jjdU3qD4AOIhIjIuWAtsD6MOZkTLE5eBA+W1qR66JnI9dc7XU6xgTN05rUqrpeRD4FVgM5OIek1oYrJ2OK08dzcsjIieW6znugYkWv0zEmaCWhJvXTwNPhzMMYL7z/n72cSTaXDGvtdSrGhMSG2jAmDI4dg08WVaJ3zEdE/eFKr9MxJiSFdhAico6IxLvPO4vICPcGN2NMPr6Yn01aZhl6t9sL5ct7nY4xIQlkD+JdIFtEGgITgQbA/8KalTERbtarO6lAKpff1dTrVIwJWSAdRI6qZgHXAs+r6l+AWuFNy5jIlZMDs7+sSK+Y+cRf08PrdIwJWSAdRKaIDAAGAXPc12LDl5IxkW3pt5nsOppAn4uToUwZr9MxJmSBdBBDgEuBf6jqFhFpALwV3rSMiVyzXtxGDJlceffZXqdizCkp9DJXdUZfHeEzvQUYG86kjIlks+aVpXPMNyRcd7nXqRhzSvLtIERkhqreICJrgJNG9FPVlmHNzJgItG1zDBsP1WZ4288hPt7rdIw5JQXtQfzZ/fcPxZGIMaeDH97JAOCau84qZEljSr58OwhVTXafllefIj/g3A8B/BrGvIyJSF8vPpM20Ss5q/9lXqdizCkL5CT1DBF5QBxlReQl4F/hTsyYSLNry1FWHDqX3q23Qqxd6GciXyAdRFucYbsX4YzQuhMI6OdRYTWp3TuzD7o1qVeKyCPBJG9MSTLn6fUoUfS+40yvUzGmSAQyWF8mcBQoC5QBtqhqTmFBgdSkdn2tqnaew0S8D2Yp9aJ+5bwhwRXtMqakCmQPYhlOB3ER0B4YICIzA4gLpCa1MaeFw7sP81lyM7rVX4XEhnWQZGOKTaE1qUXkQlVdnue1gar630LiAqlJ3RlnrKcknENX96vqj37WZTWpS1n93EiLXfVKMve8M4CXbp9Ki1vqFGvbkRrrZdulcZvDXpMap3b0zcBHASwbSE3qSkAF9/mVwKbC1ms1qYsv1su2Iy12UN3PtYrs18/mfl7sbUdqrJdtl8ZtJoSa1IEM9x0nIn1EZAaQDHQDXiskDAKoSa2qh1Q1zX3+MRArItUDWLcxJUbWgVTmbGvJVY03Ex1nJVbM6SPfT7OIdBeRScAW4Hrgv8B+VR2iqh8GsO5Ca1KLyJkiIu7zi9189oW2KcZ4Y9Gzi9lHdXrfHPphB2NKooJ+7swFzgHaq+otbqdQ6NVLx6kzRPjxmtTrgRnq1qQ+Xpcap+NZKyKrgBeB/u6ukDER44P/HSaOY/QY0cTrVIwpUgVdbtEG51f/ZyLyC85VSNHBrFwLqUmtquOAccGs05iSRA+k8MEvLeha/xcqVrbiQOb0ku8ehKr+oKoPqOo5wGPA+UCciHziXlVkTKm37tWv+JmG9OlvA/OZ009AZ9RU9Vt1Lk+tAzyPUx/CmFJv1uQUAK4e3sDjTIwpekHd0aPOHdRz3Ycxpdu+fcza1JxLav1Krdr1vM7GmCJn1+QZE6KkiXNZzoX06RvUqTljIkZBl7l+LCL1iy8VYyLL7DecK7J7/19od04bU9IVtAcxGZgnIg+JiI1dbIyvPXuYteFcmlTdw7lNxetsjAmLggoGzRCRj4BHgOUi8l987oNQ1WeLIT9jSqSU/37Ilwzkvt4HvE7FmLAp7CR1JnAYiAcqEsSNcsaczj6ZsIMsYuk9tIbXqRgTNvl2ECLSE3gWZ3iMC1T1SLFlZUxJlpzMrPWNqVk+lbaXVM4ExZwAACAASURBVPQ6G2PCpqA9iIeAfupn+G1jSrP0abP4iIHcfGUGUXYdoDmNFXQndYdT7RwKKznqs9xFIpLt1pAwpkSb/5+tHKYCfYdW9ToVY8IqbL9/fEqO9gKa4VSia5bPck9iN9+ZSJCUxHvrzyWhzFE6d/Y6GWPCK5w7yIGWHB2OU1VudxhzMaZIZE57l9lcw9U9MomL8zobY8IrnB1EHWC7z3SS+1ouEakDXEtgBYiM8dzCSZvZTzWuG1zJ61SMCbtCa1KHvGKRfkAPPbEm9cWqOtxnmXeAZ1R1sYhMBuao6kw/67Ka1KWsfm5JjI3ftYu3BuzjjZihvD9nCfHxJ1/1XRLzLqmxXrZdGrc57DWpg3ngjPg612d6NDA6zzJbgK3uIw3nMFOfgtZrNamLL9bLtktibPbYp7QWO/T6XqnF3vbpGOtl26VxmwmhJnVQo7kGKbfkKLADp/jQTXk6p9wxkn32IGaFMSdjQrb4jfUkU5vrBnqdiTHFI2wdhKpmicjxkqPRwCR1S4668+28g4kcP//MexubERedxVVXhfN3lTElR1g/6VpIydE8rw8OZy7GnAqdPoN36U+3jhlUqmQdhCkd7D5QYwKwfPJattKAfreW8zoVY4qNdRDGFGbjRqZvOp/Y6Gz69PE6GWOKj3UQxhRCp01nBjfQo0sGCQleZ2NM8bEOwphCLJmyge3U5YZby3qdijHFyjoIYwry449M33IR8TFZ9PY3UIwxpzHrIIwpQM60GbxDP3p2zaKSja5hShnrIIzJjyrfTfmJHSRyw61lvM7GmGJnHYQx+Vm9munbL6VMbBZXX+11MsYUP+sgjMlH9tvO4aVe3bOpaJVFTSlkt4Qa448qX0zZzi5qcfNtXidjjDdsD8IYf77/nv/u6kblsse46iqvkzHGG2HtIAqrSS0ivUVktYisFJHlItI+nPkYE6jDb73Pe1zHDf2UMnZ+2pRSYTvE5FOTujtONbllIjJbVdf5LPY5MFtVVURaAjOAc8OVkzEBUWXWW2kcpgK33O51MsZ4x9Oa1Kqa5hayACgPhKe8nTHBWLKEt/b1pG61NNrbPq0pxTytSQ0gIteKyAbgI8BOBxrP7Zr0MfO4glsGxxBlZ+lMKeZpTeo8y3cEHlHVbn7mWU3qUlY/17PYQ4fYcNNSRh4ew+TJS6lX70jxtV3KYr1suzRuc8TVpPYTswWoXtAyVpO6+GK9bNur2BUvvqgXsFzbNNhb7G2Xtlgv2y6N20wINanDuQOdW5NaROJwalLP9l1ARBqKiLjPLwDigH1hzMmYAu2ZtYUVtGHgn8p7nYoxnvO6JnVf4FYRyQSOAje6PZ0xxS87m1nfNiUuKpNbhtq1rcZ4WpNaVZ8EngxnDsYE6uj8b3j72PX0vWwn1arV8zodYzxn12gY45r59C+kUIU/PniG16kYUyJYB2EMQFYW//n6XBqUTaJzL6scZwxYB2EMABumLOHrzEvpfdkGnMsmjDHWQRgDTHwhlRgy6XhbtNepGFNiWAdhSr1juw8yZW0brj5rJVVq2e6DMcdZB2FKt5wcpveYyB6twbD7rSqQMb6sgzClmj76GC+s7ETTM/fTfbgNJGyML+sgTOn1/vt8O+YLVtCGPz9axU5OG5OHlRw1pdO6dXDrrTxf5QOqoAy81XoHY/KyPQhT+qSkQJ8+/BrfmPcPduGOO4Ry5bxOypiSxzoIU7pkZ8PNN8OWLbx8xQeICHfd5XVSxpRMXtekvtmtSb1aRBaJSKtw5mMMjz4KH39M2lOv8J9PEunbF846y+ukjCmZwtZB+NSk7gU0AwaISLM8i20BOqlqS+AJYHy48jGGd9+Ff/wDhg7ltayhpKTAvfd6nZQxJVc4T1Ln1qQGEJHjNanXHV9AVRf5LL8YSAxjPqY0W7sWBg2CSy7hyFPj+HdToXt3aNvW68SMKbnCWXL0eqCnnlhytK2q3p3P8vcD5x5fPs88KzlaysojFmVsTGoqbYYNIyo9ne9ff53/LWjFyy834oUXfqBly4NF1m5R5326x3rZdmnc5pJWcrQfMMFneiDwUj7LdgHWA9UKW6+VHC2+WC/bLrLYrCzVHj1UY2NVv/1Wjx5VrV1btVOnom/3VONLW6yXbZfGbSaEkqPhPMSUBPie/ksEduZdSERaAhOAXqpq5UZN0frb32DuXHj9dWjXjjdehZ074b//9ToxY0o+r2tS1wXeAwaq6k9hzMWURjNmwNix8Kc/wR13kJHhTLZrB126eJ2cMSWf1zWpHwGqAa+IM85BlgZ7jMwYf1avhiFDnN7gxRcBZydi2zYYPx4bVsOYAHhdk3oocNJJaWNORczBg3DbbZCQADNnQlwcKSnw+OPQtStccYXXGRoTGWwsJnN6ycqi2ZgxsGMHfPUV1KoFwD//Cfv3w7//bXsPxgTKOghzennwQaouXw4TJsAllwCwZQu88IJzG0Tr1h7nZ0wEsbGYzOnj7bfh6afZ0bs33H577ssPPgjR0TBmjIe5GROBrIMwp4eVK51OoX17NvuMvrdoEUybBvffD3XqeJifMRHIOggT+fbuhT59oGpVmDkTjY0FICMD/vhHZzC+kSM9ztGYCGTnIExky8qCG2+EXbvg66+hZk1Yvx5w7nlYtw7mzIGKVm7amKBZB2Ei2wMPwBdfwBtvwEUX5b68bp1zzmHAALjqKg/zKyKZmZkkJSWRnp7ud37lypVZ73aMwfIq1su2T+dtLlOmDImJicS6e9KnwjoIE7neeguefRaGD4fBg3NfzslxDi1VrAjPP+9dekUpKSmJihUrUr9+fcTPdbqpqalUDHE3yatYL9s+XbdZVdm3bx9JSUk0aNAg1BRz2TkIE5lWrHB6gU6d4JlnTpg1c2YiixbBc8/BGWd4lF8RS09Pp1q1an47B2OOExGqVauW755msKyDMJFnzx649lqoUcMZb8lnV3rJEhg//mz69IGBAz3MMQysczCBKMrPiXUQJrJkZsINN8Du3fD++yfsIhw44JyvrlHjGJMm2R3TRe14HYKdO3dy/fXXe5xN/u655x4WLlzod17Pnj1JSEigX79+J7y+ZcsW2rZtS6NGjbjxxhvJyMg4KXbfvn106dKFWrVqcffdJ5a1+f777znvvPNo2LAhI0aMOF7G4AQbNmzg0ksvpXr16vz73/8+Yd6nn35KkyZNaNiwIWPHjs19/f777+eLL74IeNuLmtc1qc8Vke9E5JhbMMiYgo0cCQsWOCPutWmT+7KqcxvEjh3w8MPrqFLFuxRPd7Vr12bmzJlep+HX/v37Wbx4MR07dvQ7f+TIkfzXz1jvDzzwAH/5y1/YtGkTVapUYeLEiSctU6ZMGZ544gnG+Lnj8s4772T8+PFs2rSJTZs28emnn560TNWqVXnxxRcZMWLECa9nZ2dz11138cknn7Bu3Trefvtt1q1zCm8OHz78hA6juHldk3o/MAL4N8YU5s03nTEz7rnnpONHzzzj7FCMHQvNmqV6lGDpsHXrVlq0aAHA1KlTue666+jZsyeNGjVi1KhRgPOlN3jwYFq0aMF5553Hc889B0Dnzp255557aNeuHS1atGD58uUALF26lHbt2nH++efTrl07Nm7cmLue+++/n/POO4+WLVvy0ksvAc4v9l69etGmTRt69OhBcnIyADNnzqRnz5755t61a9eTTvKqKl988UXuXtGgQYOYNWvWSbHly5enffv2lClT5oTXk5OTOXToEJdeeikiwq233uo3/owzzuCiiy4iJubEa4OWLl1Kw4YNOfvss4mLi6N///588MEHANSrV499+/axa9eufLcpnLyuSb0b2C0ip8GFiCasli+HO+5wCjk8/fQJs955x9mx6NcP7r3XGaPvtHbPPc6d4z7KZmc744mEoGx2trM3FuIlXytXruSHH34gPj6eJk2aMHz4cHbv3s2OHTtYu3YtACkpKbnLHz58mEWLFrFw4UKGDRvGunXrOPfcc1m4cCExMTF89tlnPPjgg7z77ruMHz+eLVu28MMPPxATE8P+/fvJzMxk+PDhTJ06lQYNGjB9+nQeeughJk2axLfffhv04a99+/aRkJCQ+8WdmJjIjh07Ao7fsWMHiYmJudOhxJ911u+11RITE1myZEnu9AUXXMC3335L3759A15nUQlnB1EH2O4znQSEVCI+T01qFixYEFJCaWlpIceeanwkxnrZtm9s7P79tBk2DBIS+H7ECDK/+SZ3uTVrKnHffa1p0SKVoUNX8dVXOafl+1W5cmVSU509o/iMDKKys09cQJWsvK8FSpWMjAyOpRa+55WamkpaWho5OTmkpqaSk5NDx44diYqKIjMzk8aNG7N+/XrOPfdcNm/ezJ/+9Cd69OhB165dSU1NJTs7m969e5Oamsr555/PoUOH2L59O2lpaYwaNYqff/4ZESEzM5PU1FQ+/fRTbrvtNo4ePQpAbGwsK1asYO3atVxzzTWICNnZ2dSsWZPU1FS2b99OuXLlct8rf44cOYKq5i5zfDuOT6elpZ0wP6+cnBwyMjJOWD47Ozt3+siRIydMn/x2K8eOHTth+ePbC3D06NETphMSEvjll19y37+Ctu249PT0U/ocHxfODsLfKcKTz9wEQFXHA+MBmjRpop07dw4poQULFhBq7KnGR2Ksl23nxmZmQrdukJYG337LZeefn7vMTz/B9ddD/fqwYEFlqlXr6GnO4Wx7/fr1vx8aeeWVk+YXxbX1cQEsW7FiRSpUqEBUVBQVK1YkKiqKChUq5LYdHx9PXFwcdevWZc2aNcydO5c33niDOXPmMGnSJKKjoylfvnzu8iJCpUqVePjhh+nevTsffvghW7dupXPnzlSsWPGk5QHKlStH8+bNmTdv3knbfDymYsWKLFmyhD/96U8A/P3vf+eaa67JjReR3NgKFSpw6NAhypYtS0xMDCkpKSQmJub7fkZFRREXF5c7v0mTJiQnJ+dO79+/n7p16+YbLyLEx8fnzm/UqBFTp07Nnd63bx/169fPnc7JyaFKlSpUrFgx4P/nMmXKcL7P30qownmSOqCa1MYU6N57YeFCZ/hunw/8hg3QuTNERcEnn0C1at6laE62d+9ecnJy6Nu3L0888QQrVqzInTd9+nQAvvnmGypVqkTlypU5ePAgddzRFCdPnpy77BVXXMFrr71GVlYW4Hz5NmnShD179uQehsnMzOTHH38EoGnTpmzevBmAtm3bsnLlSlauXJnbOfgjInTp0iX3xPuUKVPo3bs34JwfuPXWWwvc1lq1alGxYkUWL16MqvLmm2/mxr///vuMHj26wPiLLrqITZs2sWXLFjIyMpg2bdoJ+f7000+553yKm6c1qY0p0BtvwLhxcN99cNNNuS+vXevcH5eTA19+Ceec42GOxq8dO3bQuXNnWrduzeDBg/nXv/6VO69KlSq0a9eOYcOGMW7cOABGjRrF6NGjueyyy8j2OVQ2dOhQ6tatS8uWLWnVqhX/+9//iIuLY+bMmTz66KO0atWK1q1bs2jRIgCuuuqqAg+tdOjQgX79+vHVV1+RmJjI3LlzAXjyySd59tlnadiwIfv27eN2d7j4bdu2UbZs2dz4+vXrM3r0aCZPnkxiYmLu1UavvvoqQ4cOpWHDhpxzzjn06tULgJ9//plKlSoBsGvXLhITE3n55ZcZM2YMiYmJHDp0iJiYGMaNG0ePHj1o2rQpN9xwA82bNweczm/z5s1ceKFHlZhVNWwP4ErgJ+Bn4CH3tWHAMPf5mTh7GoeAFPd5pYLW2bhxYw3Vl19+GXLsqcZHYqyXbS9/5RXVuDjVrl1VMzN/f325arVqqrVrq27YUPTtltT3a926dQXGHjp0KOR2izO2U6dOumzZsrC2fdlll+mBAwdCis3r/vvv11WrVoUUq6p688036+7du0OOf++99/Rvf/tb0LH+Pi/Acg3yO9zrmtS7cA49GfO7Xbto8cgjULs2TJ8O7tUlM2Y4Qy7VqAGffw4NG3qbpimZnnnmGbZt20ZCQsIpr+vpPFfMBeutt946pfisrCzuu+++U1rHqbDB+kzJkpEB119PTGoqfPYZVKtGTg488gj84x/Qrh28954zqreJPEVxZU1h2rYN6WLJEinvHd/FzYbaMCXLPffAt9+ycdQoaNWKnTud4br/8Q/nTukvvrDOwZjiYnsQpuSYMAFefRVGjWL35Zezehr83/9BerpzZeewYTa+kjHFyToIUzIsXgx33QVXXMEvQ//JI7fv5+uvoW1bZ4SNxo29TtCY0scOMRnvJSfDdddxqPa5/PXcWTRtEc2yZVX55z/hm2+sczDGK9ZBGG8dO8b+awbz97130uDA9zz5Yln694c331zC6NG5FzCZEsCG+y7+4b73799P9+7dadSoEd27d+fAgQMArFmzhsE+VRTDxToIU/wOHYJFi9jw+HTuPfdj6i2fyaOZD3NZxxiWLoUpU6BGjZP/QE3JYMN9F99w32PHjqVr165s2rSJrl275o6Ke95555GUlMS2bdsKf1NOgXUQJnyys2HjRme41Ycfhj59OFCvNZMrj6DDZdk0fexGXtr6B65utY1Vq2D2bLjoIq+TNoWx4b6Lb7jvDz74gEGDBuXmNWfOnNy4q6++mmnTpuW7rUXBduBN0di7F1av/v2xZg0dVq+GjAy2Uo+50ov3y93H50cuJYsYGtVO48lbDzDozwnUPLO519lHFD+jfZOdXTbU0b7Jzi57KqN923DfYRzu+7fffqNWrVqAM+bT3r17c5e78MILGTt2bG6nHA7WQZjgHDvmjJTn2xmsXg1uQRMFNle5mO9qXc+8xH+wOO0Sft5dCRTOrgn3Xg99+8JFF1WwS1ZPE127dqVy5coANGvWjF9//ZXmzZvzyy+/MHz4cK666iquuOKK3OUHDBgAQMeOHUlNTSUlJYXU1FQGDRrEpk2bcof7Bvjss88YNmxY7pd31apVWbt2LWvXrqV3795ERUWRnZ2d+yWanJxMjRo1gsrf3/mCYOo6exV/xhlnsHNneMc/tQ7C+KcKSUlUXbzYuQT1eEewcSO4I2vuiz2Tnxr0YMPZ/Vjd+HxWH6rPqm1V2Lc/Cg5AuXJZXH55DCO6OyN2N21q9zEUBX+/9FNTj57CcN+hx4IzxPdx0dHRZGVlUaVKFVatWsXcuXN5+eWXmTFjBpMmTQJO/vITER5++GG6dOnC+++/nzvcNzhfnnmXV9V8h/suW7Ys6enpAPkO951X9erVSUlJISsri5iYGJKSkqhdu3bA25+YmEhSUlLudCjx27f/XjrHN75mzZokJydTq1YtkpOTqV69eu5y6enpJwwkGA5e16QWEXnRnb9aRC4IZz4mH2lpTicwfjwMH05Ox87sSziHDXW7kzL6Y6aNXsmznzTlvqNPcP05P3Dh2fuoVjmT6pnJtPtpMrctGsrry9qQFleNa6+LYvx4WLMGZs/+hg8/hBEjoFkz6xxKExvuu2iG+77mmmuYMmVKbl5XXfV78c3iGAY8bHsQPjWpu+OM0rpMRGar6jqfxXoBjdxHW+BVQqw6F9FUnRO6mZnOIyMj97lmZBK3ZTvHKq4lOz2DrPRsso9lkXk0i6xj2WSmO4+Mo9lkpOeQkZ7DsXTNfWzduouklw9yNF04kh7FkWPRpKXHkHYshrSD2Rzam8mhw1GkkMABOrOfvqRQmey8H40UKHsM6tWDeo3gwu7O/QnHH+ecc3LFS5/DpaaU2bFjB0OGDCEnJwfA73Dfhw4dOmG470GDBvHss89y+eWX5y47dOhQfvrpJ1q2bElsbCx//OMfufvuu5k5cyZ33XUX9913H1lZWdxzzz00b96cq666itdff52hQ4f6zatDhw5s2LCBtLQ0EhMTmThxIj169ODJJ5+kf//+/O1vf+P8888vcLjvgwcPkpmZyaxZs5g3bx7NmjXj1VdfZfDgwRw9epRevXrlO9z3hRdeyKFDh4iKiuL5559n3bp1VKpUKXe47+zsbG677bbc4b7/+te/csMNNzBx4kTq1q2buxcG8OWXX57QYYSD+Dv+VSQrFrkUeExVe7jTowFU9V8+y7wOLFDVt93pjUBnVU3Ob71NmjTR41c4BGPuP5bz58crE+X+jFXfgnfq9+kJy6gKqjkgUbmvK4KqnBCnyO/zVdxp53mOG6sq5CCoQg5Rfh/ZRJNNNBqGnbxosigvR6gQfZTKZTOpXBkq14ilSp3yVEksT9VqQo0azqipycmr6NWrFbVqQUJCcHsBnlejK+bYcLa9fv16mjZtmm9sUVSUK47Yzp078+9//zu3vkE42m7fvj1z5swpcDTXQNsdOXIkAwcOpGXLlkHHAtxyyy0899xzJ5wXKYptPnbsGJ06deKbb7456aoo8P95EZHvVTWowhLh7CCuB3qq6lB3eiDQVlXv9llmDjBWVb9xpz8HHlDV5XnW5VuTus2MGTOCzmfrh3t5d0pVoqKE3Gqo4m/bxflKP76IzxzVHKKiokB+7zpEjncBv395is+6ndYU1RxiosldTqIgKhqIEqKiQKJBogSJFneeINHOMjlkExcfQ1Q0RMVGER2rRMVAdIwQHQcxsUJULMSWEWLiICbeeR5bFjJz0qlcrSxx5YT4slCmTA6xsTkBf9GnpaXl3iAVrNIWG862K1euTMMCxjfPzs4mOsTLmIoz9sorr2TMmDFccMEFYWt72bJllC1btsDDL169X0XV9ubNm0lOTqZDhw5+l9u8eTMHDx484bUuXboE3UGEs1hQP2CCz/RA4KU8y3wEtPeZ/hxoU9B6rWBQ8cV62XYkxoaz7dOlYFBJaft03+aiKhjkdU1qq1ttjDEllNc1qWcDt7pXM10CHNQCzj8YU5ppmA4Hm9NLUX5OwnYVk6pmicjdwFwgGpikqj+KyDB3/ms45UivBDYDR4Ah4crHmEhWpkwZ9u3bR7Vq1YK6CcuULqrKvn37ThoOJFRe16RW4K5w5mDM6eD4zVh79uzxOz89PT3kLwWvYr1s+3Te5jJlypww9MepsDupjYkAsbGxNGjQIN/5CxYs4Pzzzw9p3V7Fetl2adzmUNhorsYYY/yyDsIYY4xf1kEYY4zxK2x3UoeLiKQCwY+14agOnMoIQacSH4mxXrYdibFeth2JsV62XRq3uYmqBjfGR7B31nn9IIS7AYsi1su2bZsjIzZS87b3y7Y5v4cdYjLGGOOXdRDGGGP8isQOYrxHsV62bdscGbFeth2JsV62bdscgIg7SW2MMaZ4ROIehDHGmGIQMR2EiPQTkR9FJEdELvR5vZqIfCkiaSIyLphYd95otyb2RhHpUUgOrUTkOxFZIyIfikilILehtYgsFpGVIrJcRC4OIna6G7dSRLaKyMog2x7ubuOPIvJUEHGPicgOn7avDKZddx33i4iKSPXClz4h7gm3VvlKEZknIgFXgheRp0Vkgxv/vojkX17s5Nh8Py8FxBRYf72Q2EkisltE1gYT58ae5X7+17s5/zmI2DIislREVrmxj4fQfrSI/OAW/wombqv7d7RSRJYXHnFCbIKIzHT/f9e71SsDjW3i81leKSKHROSeIOL/4r5Xa0XkbREJeFAlEfmzG/djIG36+1yISFURmS8im9x/qwQRG/TnOuRLtYr7ATQFmgALgAt9Xi8PtAeGAeOCjG0GrALigQbAz0B0ATksAzq5z28DnghyG+YBvdznV+KUWw3lvXgGeCSI5bsAnwHx7vQZQcQ+Btx/Cv9vZ+GM6PsrUD3I2Eo+z0cArwURewUQ4z5/EnjyVD9rBSwf7X52zgbi3M9UsyDa6whcAKwN4f2tBVzgPq8I/BRo2zgFDyu4z2OBJcAlQbZ/L/A/YE6QcVuD/Tz4xE4BhrrP44CEENcTDewC6gW4fB1gC1DWnZ4BDA4wtgWwFiiHMwbeZ0CjYD8XwFPAX93nf83vc51PbFCfa9UIusxVVder6kk3yKnqYXVKlqYHGwv0Bqap6jFV3YIz7HhBv+qbAAvd5/OBvgFvgJsKcHyvozIhFEcSZ6znG4C3gwi7E6e06zEAVd0dbLun4DlgFCeW+w6Iqh7ymSwfzDpUdZ6qZrmTi3GKUQUam9/nJT8XA5tV9RdVzQCm4Xy2Am1vIbA/iPZ8Y5NVdYX7PBVYj/NFFkisqmqaOxnrPgJ+j0UkEbgKmBBU0qfA3WvvCEwEUNUMVU0JcXVdgZ9V9dcgYmKAsiISg/NlH+jfcFNgsaoecT+XXwHXFhSQz+eiN04Hiftvn0BjQ/hcR04HESZ1gO0+00kU/Me1FrjGfd6PE6vhBeIe4GkR2Q78GxgdZDxAB+A3Vd0URExjoIOILBGRr0TkoiDbvNs9VDMpv11af0TkGmCHqq4Ksj3fdfzDfb9uBh4JcTW3AZ+EmkMAgv0chYWI1AfOx9kTCDQm2j1cuRuYr6oBxwLP43T+OUHEHKfAPBH5Xpya84E6G9gDvOEe2pogIuVDaB+cImYB/9BS1R04f7fbgGScAmfzAgxfC3QU55B4OZwjCMF+fwDUVLeomvvvGSGsI2AlarhvEfkMONPPrIdU9YNCwkcCZ4pI5yBi/VVeeUREHvOXA84XzYsi8ghONbyMk1ZYwDbg/GL5i6q+KyI34PwK6hZIrM82DMDPh7qQdmOAKsAlwEXADBE5W939zkJiXwWewPmDfgLn8NZtAbb7IM6hnnwVts2q+hDwkIiMBu4GHg001l3mISALmBpMuwXl7G8z/LxWrJcHikgF4F3gnjx7XgVS1WygtXuO5n0RaaGqhZ4LEZE/ALtV9Xs/f3OBuExVd4rIGcB8Edng/uotTAzOoZPhqrpERF7AOdTycDCNi1Pl8hqC+JHm/jjqjXM4OgV4R0RuUdW3CotV1fUi8iTOkYc0nMOQWQVHlQChHLvz8kE+x8+AweRzDiK/WJwPx2if6bnApQHm0RhYGmTuB/n90mIBDgUZHwP8BiQGOpAYBQAABKdJREFUGfcp0Nln+megRgjvfX0CPE4OnIfzq3Sr+8jC+eV1Zoj/7/UCbdsnZhDwHVCuKD9rfpa7FJib3+eqqN9bP7Gx7mf33lDifdbzKAGebwL+hbOntBXnOP4R4K0Q230siHbPBLb6THcAPgqhzd7AvCBj+gETfaZvBV4JcZv/CfxfsJ8LnHHoarnPawEbg/1MBfq5Vo2gcxBhMhvoLyLxItIAaAQszW9h99cOIhIF/A14Lb9l87ET6OQ+vxwI5jAROHsbG1Q1Kci4WW57iEhjnBN7AQ34JSK1fCavxdlVLpSqrlHVM1S1vqrWx/kyuUBVdwWatIg08pm8BtgQRGxP4AHgGlU9EmhciAKpvx4W7jmpicB6VX02yNga7p4DIlIW9/MVSKyqjlbVRPf/tj/whareEmC75UWk4vHnOHuZgX6udgHbRaSJ+1JXYF0gsXn43RMvxDbgEhEp577vXXHO+QTE5/ujLnBdCO2D87ka5D4fBAS7txucUH9tFPcD58spCTiG8yva9xfbVpwTMmnuMs2CiH0I5xf1RtwrjArI4c84V4n8BIzF3RsIYhvaA9/j7F4uAdoEGT8ZGBbCexcHvIXzR7gCuDyI2P8Ca4DVOB/OWiH+/20l+KuY3nVzXg18CNQJInYzznmBle4jmCug8v28FBBzpfu5+BnnMFUw2/k2zjHtTLfd24P8TKn7Hh3f1isDjG0J/ODGriWIK+PyrKczQVzFhHMeYZX7+DGE96s1sNzNexZQJcj4csA+oHII2/o4Tie61v3biA8i9muczmwV0DWUzwVQDfgc58fl50DVIGKD/lzbndTGGGP8Ku2HmIwxxuTDOghjjDF+WQdhjDHGL+sgjDHG+GUdhDHGGL+sgzDGhzijo24RkarudBV3ul4+y18rzki15waw7gtF5MWiztmYcLHLXI3JQ0RGAQ1V9Q4ReR3nzt1/5bPsDJw7Wj9X1cf+v707Zm0qDKM4fo5fwUFFxMFFoUPpqtAOdnBTcDDOOpVKEXFwcVVEu7WDg+DkIp0EQbcquEnXjv0ArkUcjsPzStObFxW1IeD/t4TcvCHJ9HBz73vOFL8mcOQ4gwAmrat2zK6pNqI97S1q+UcXVZuQbowdv2b7vcsp27u2T9pecutNsL3og06Czz92FgOzhAEBDCT5pgp/XFeF302EMjZXJb1Nsivpi+2F9v4tVT7RiqTnkh5mMmLknqSVJPOqPKH9f/9LgL/DgAD6rqiiCuZ+smak6n5QexyNvbaqCu37mqSXufNR0jPbd1SFN7Of7In/zkzFfQOzwPa8pGVVPPoH26/SMvjH1hxXBSDO2Y6qnSy276cu7J1W9SScsH0syaHOhCSPbL9RZTh9sn05yW+HEQLTwBkEMKaldG6q/lrak/REVRIzdF3SyyRnU4m1Z1R1lJda29gLSTdVaZ93O59zLpV4+1gVPPfLu6CAaWNAAIfdlrSX5F17viHpvO3FwbqRpK3BsdeqofBA0naSbdVwuGX7wmDtmqvAfkd1/eEoG++AP8JtrgCALs4gAABdDAgAQBcDAgDQxYAAAHQxIAAAXQwIAEAXAwIA0MWAAAB0fQe7n77A2gJ+tQAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![index.png](attachment:index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We set the threshold** \n",
    "$\\hat{Y}$ = 0 if prob < 0.5 else 1 if prob > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing sigmoid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for scientific computation \n",
    "import pandas as pd # for working with data\n",
    "from sklearn.model_selection import train_test_split # for splitting the data\n",
    "from sklearn.datasets import load_breast_cancer # dataset that we will be using \n",
    "import warnings # optional \n",
    "warnings.filterwarnings( \"ignore\" ) # optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "    '''\n",
    "    input:  \n",
    "        z : a scalar or an array \n",
    "    output: \n",
    "        h : sigmoid of z \n",
    "    '''\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function For Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for a single training example is\n",
    "$$ J(\\theta) = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)  $$ \n",
    "\n",
    "* when the model predicts 1 and the label $y$ is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1and the label is 0, the second term of the log loss becomes a large.\n",
    "\n",
    "\n",
    "***Cost Function for m training examples***\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} ( \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) ) $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent [ Optimization Objective ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our weights $\\Theta$ we apply our gradient descent, to improve our weights at every iteration. We take out the partial derviative of our cost function, or in other words, how much the cost function will change if we change $\\Theta$ little bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(x^{(i)}-y^{(i)})x_j $$\n",
    "\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* https://nipunbatra.github.io/ml2023/lectures/logistic-1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are taking gradient step to reach to our global minimum, so that's why we are updating our previous theta parameter with the new one.\n",
    "\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical Note on $\\alpha$ hyperparameter**\n",
    "\n",
    "\n",
    "You have to tune the learning rate means you have to try different different rates and see your model how it performs. \n",
    "\n",
    "If you choose **very small learning rate**, it will be very very slow and it will never converge to the local minimum.  \n",
    "\n",
    "If you choose **very large learning rate**, your model might diverge and never converge to the local minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide on implementing vectorized Logistic Regression ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis Function**\n",
    "\n",
    "**z**= $ X . \\Theta\\$\n",
    "<br>\n",
    "And then you apply the sigmoid to each element in 'z': ℎ(𝑧)=𝑠𝑖𝑔𝑚𝑜𝑖𝑑(𝑧)\n",
    "\n",
    "- It has dimensions (m,1), where m is no. of training examples.   \n",
    "<br>\n",
    "\n",
    "**Vectorized Cost Function**\n",
    "$$J(\\Theta)= \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "\n",
    "\n",
    "**Vectorized Gradient Descent** \n",
    "<br>\n",
    "Above, we are taking out the gradient and updating  single weight $\\theta_i$ at a time, but it will consume lot of time and space, instead of doing that, we can make a giant vector theta, which will contains all the feature weights, as given below:- \n",
    "\n",
    "$$\\mathbf{\\Theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$ \n",
    "<br> \n",
    "<br> \n",
    "\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1)  \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "\n",
    "\n",
    "We have to update our theta values, so it is also vectorized:- \n",
    "\n",
    "$$\\mathbf{\\Theta} = \\mathbf{\\Theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        theta: your final weight vector\n",
    "    '''\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z =  np.dot(x, theta) \n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z) \n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h))) \n",
    "\n",
    "        # update the weights theta\n",
    "        theta =  theta -  alpha/m * (np.dot(x.T, (h-y))) \n",
    "        \n",
    "    return theta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  ans  \n",
       "0          0.4601                  0.11890    0  \n",
       "1          0.2750                  0.08902    0  \n",
       "2          0.3613                  0.08758    0  \n",
       "3          0.6638                  0.17300    0  \n",
       "4          0.2364                  0.07678    0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import  train_test_split\n",
    "data1 = pd.read_csv(r'C:\\Users\\HRUSHI\\OneDrive - iitgn.ac.in\\ML\\Breast Cancer Detection System\\breast-cancer.csv')\n",
    "# data.head()\n",
    "d1 = load_breast_cancer()\n",
    "X = d1.data\n",
    "y = d1.target\n",
    "data = pd.DataFrame(X,columns=d1.feature_names)\n",
    "data[\"ans\"]=y\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean radius  mean texture  mean perimeter    mean area  \\\n",
      "count   569.000000    569.000000      569.000000   569.000000   \n",
      "mean     14.127292     19.289649       91.969033   654.889104   \n",
      "std       3.524049      4.301036       24.298981   351.914129   \n",
      "min       6.981000      9.710000       43.790000   143.500000   \n",
      "25%      11.700000     16.170000       75.170000   420.300000   \n",
      "50%      13.370000     18.840000       86.240000   551.100000   \n",
      "75%      15.780000     21.800000      104.100000   782.700000   \n",
      "max      28.110000     39.280000      188.500000  2501.000000   \n",
      "\n",
      "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
      "count       569.000000        569.000000      569.000000           569.000000   \n",
      "mean          0.096360          0.104341        0.088799             0.048919   \n",
      "std           0.014064          0.052813        0.079720             0.038803   \n",
      "min           0.052630          0.019380        0.000000             0.000000   \n",
      "25%           0.086370          0.064920        0.029560             0.020310   \n",
      "50%           0.095870          0.092630        0.061540             0.033500   \n",
      "75%           0.105300          0.130400        0.130700             0.074000   \n",
      "max           0.163400          0.345400        0.426800             0.201200   \n",
      "\n",
      "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
      "count     569.000000              569.000000  ...     569.000000   \n",
      "mean        0.181162                0.062798  ...      25.677223   \n",
      "std         0.027414                0.007060  ...       6.146258   \n",
      "min         0.106000                0.049960  ...      12.020000   \n",
      "25%         0.161900                0.057700  ...      21.080000   \n",
      "50%         0.179200                0.061540  ...      25.410000   \n",
      "75%         0.195700                0.066120  ...      29.720000   \n",
      "max         0.304000                0.097440  ...      49.540000   \n",
      "\n",
      "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
      "count       569.000000   569.000000        569.000000         569.000000   \n",
      "mean        107.261213   880.583128          0.132369           0.254265   \n",
      "std          33.602542   569.356993          0.022832           0.157336   \n",
      "min          50.410000   185.200000          0.071170           0.027290   \n",
      "25%          84.110000   515.300000          0.116600           0.147200   \n",
      "50%          97.660000   686.500000          0.131300           0.211900   \n",
      "75%         125.400000  1084.000000          0.146000           0.339100   \n",
      "max         251.200000  4254.000000          0.222600           1.058000   \n",
      "\n",
      "       worst concavity  worst concave points  worst symmetry  \\\n",
      "count       569.000000            569.000000      569.000000   \n",
      "mean          0.272188              0.114606        0.290076   \n",
      "std           0.208624              0.065732        0.061867   \n",
      "min           0.000000              0.000000        0.156500   \n",
      "25%           0.114500              0.064930        0.250400   \n",
      "50%           0.226700              0.099930        0.282200   \n",
      "75%           0.382900              0.161400        0.317900   \n",
      "max           1.252000              0.291000        0.663800   \n",
      "\n",
      "       worst fractal dimension         ans  \n",
      "count               569.000000  569.000000  \n",
      "mean                  0.083946    0.627417  \n",
      "std                   0.018061    0.483918  \n",
      "min                   0.055040    0.000000  \n",
      "25%                   0.071460    0.000000  \n",
      "50%                   0.080040    1.000000  \n",
      "75%                   0.092080    1.000000  \n",
      "max                   0.207500    1.000000  \n",
      "\n",
      "[8 rows x 31 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.DataFrame(data)\n",
    "print(data.describe())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "ans                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        theta: your final weight vector\n",
    "    '''\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # get z, the dot product of x and theta\n",
    "        # print(\"Iteration:\", i+1)\n",
    "        # print(\"Type of x:\", type(x), x.shape)\n",
    "        # print(\"Type of theta:\", type(theta), theta.shape)\n",
    "        # print(\"Values of x:\", x)\n",
    "        # print(\"Values of theta:\", theta)\n",
    "        z = np.dot(x, theta) \n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z) \n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
    "        \n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha / m) * np.dot(x.T, (h - y))\n",
    "        \n",
    "    return theta \n",
    "\n",
    "# Assuming X has shape (m, n) and Y_train has shape (m,)\n",
    "# Append a column of ones to X_train for the bias term\n",
    "#data = np.c_[np.ones((data.shape[0], 1)), data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta0</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   theta0  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0       1        17.99         10.38          122.80     1001.0   \n",
       "1       1        20.57         17.77          132.90     1326.0   \n",
       "2       1        19.69         21.25          130.00     1203.0   \n",
       "3       1        11.42         20.38           77.58      386.1   \n",
       "4       1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   mean symmetry  ...  worst texture  worst perimeter  worst area  \\\n",
       "0         0.2419  ...          17.33           184.60      2019.0   \n",
       "1         0.1812  ...          23.41           158.80      1956.0   \n",
       "2         0.2069  ...          25.53           152.50      1709.0   \n",
       "3         0.2597  ...          26.50            98.87       567.7   \n",
       "4         0.1809  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  ans  \n",
       "0          0.4601                  0.11890    0  \n",
       "1          0.2750                  0.08902    0  \n",
       "2          0.3613                  0.08758    0  \n",
       "3          0.6638                  0.17300    0  \n",
       "4          0.2364                  0.07678    0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data.insert(0,'theta0',np.ones((data.shape[0], 1)) )\n",
    "theta0_column = pd.DataFrame({'theta0': [1] * (data.shape[0])})\n",
    "\n",
    "# Concatenate the original DataFrame with the new column\n",
    "new_data = pd.concat([theta0_column, data], axis=1)\n",
    "\n",
    "new_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta0</th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   theta0  mean radius  mean texture  mean perimeter  mean area  \\\n",
       "0       1        17.99         10.38          122.80     1001.0   \n",
       "1       1        20.57         17.77          132.90     1326.0   \n",
       "2       1        19.69         21.25          130.00     1203.0   \n",
       "3       1        11.42         20.38           77.58      386.1   \n",
       "4       1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   mean symmetry  ...  worst texture  worst perimeter  worst area  \\\n",
       "0         0.2419  ...          17.33           184.60      2019.0   \n",
       "1         0.1812  ...          23.41           158.80      1956.0   \n",
       "2         0.2069  ...          25.53           152.50      1709.0   \n",
       "3         0.2597  ...          26.50            98.87       567.7   \n",
       "4         0.1809  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  ans  \n",
       "0          0.4601                  0.11890    0  \n",
       "1          0.2750                  0.08902    0  \n",
       "2          0.3613                  0.08758    0  \n",
       "3          0.6638                  0.17300    0  \n",
       "4          0.2364                  0.07678    0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n",
      "The resulting vector of weights is [0.03439443, 0.26607621, 0.42755222, 1.57201338, 0.96269165, 0.00251355, -0.00043216, -0.00378767, -0.00170031, 0.00471065, 0.00199218, 0.00015272, 0.0301271, -0.0101774, -0.79573317, 0.00017769, 2.66e-06, -0.00018819, 2.56e-05, 0.00048462, 6.265e-05, 0.27777251, 0.54217099, 1.58116185, -1.18056162, 0.00321099, -0.00248597, -0.00718289, -0.00170132, 0.00676345, 0.00202168]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize weights vector theta with size (n + 1,)\n",
    "\n",
    "print(new_data.shape)\n",
    "data = pd.DataFrame(new_data)\n",
    "X = new_data.iloc[:,:-1]\n",
    "y = new_data.iloc[:,-1]\n",
    "theta = np.zeros(X.shape[1])\n",
    "# #print(y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y,test_size = 0.3, random_state=0 )\n",
    "### Now, we will train our model with gradient descent ### \n",
    "theta = gradientDescent(X_train, Y_train,theta, 0.001, 700)\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above, the you got the feature weights of all the features. Now using that you will make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(xx, theta): \n",
    "    '''\n",
    "    input: \n",
    "        X : input  \n",
    "        theta : feature weights \n",
    "    output: \n",
    "        Y : 0 Or 1 \n",
    "    '''\n",
    "    Z = 1 / ( 1 + np.exp( - ( xx.dot( theta )) ) )        \n",
    "    Y = np.where( Z > 0.5, 1, 0 )        \n",
    "    return Y\n",
    "\n",
    "y_pred = predictions(X_test, theta) ### Predicting on the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnp.squeeze(Y_test): \\nThis part squeezes the true labels Y_test if there's any unnecessary dimension. It ensures that Y_test is a 1-dimensional array, which is required for comparison with y_pred.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (y_pred==(Y_test)).sum()/len(X_test)\n",
    "accuracy\n",
    "'''\n",
    "np.squeeze(Y_test): \n",
    "This part squeezes the true labels Y_test if there's any unnecessary dimension. It ensures that Y_test is a 1-dimensional array, which is required for comparison with y_pred.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, theta):\n",
    "    y_hat = [] # making an empty list \n",
    "    for x in range(test_x.shape[0]):\n",
    "        #print(x)\n",
    "        # get the label prediction \n",
    "        y_pred = predictions(test_x.iloc[x,:], theta) \n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "    accuracy = (y_hat==np.squeeze(test_y)).sum()/len(test_x)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.8830\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_logistic_regression(X_test, Y_test, theta)\n",
    "print(f\"Logistic regression model's accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing other algorithms using SKlearn Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Accuracy  Precision    Recall\n",
      "Logistic Regression  0.953216   0.990196  0.935185\n",
      "Decision Tree        0.906433   0.969388  0.879630\n",
      "Random Forest        0.964912   0.981132  0.962963\n",
      "Gradient Boosting    0.982456   0.981651  0.990741\n",
      "SVC                  0.923977   0.899160  0.990741\n",
      "KNN                  0.947368   0.962617  0.953704\n",
      "XGBoost              0.976608   0.981481  0.981481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create an empty dictionary to store results for each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, Y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, pred)\n",
    "    precision = precision_score(Y_test, pred)\n",
    "    recall = recall_score(Y_test, pred)\n",
    "    # Store the results for the model\n",
    "    results[name] = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall}\n",
    "\n",
    "# Create DataFrame for results\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "# Define hyperparameter grids for each classification model\n",
    "models_grid = {\n",
    "    \"Logistic Regression\": {\"C\": [0.1, 1, 10]},\n",
    "    \"Decision Tree\": {\"max_depth\": [None, 5, 10], \"min_samples_split\": [2, 5, 10]},\n",
    "    \"Random Forest\": {\"n_estimators\": [100, 200], \"max_depth\": [None, 5, 10], \"min_samples_split\": [2, 5, 10]},\n",
    "    \"Gradient Boosting\": {\"n_estimators\": [100, 200], \"learning_rate\": [0.01, 0.1, 0.5], \"max_depth\": [3, 5]},\n",
    "    \"SVC\": {\"kernel\": ['rbf', 'sigmoid'], \"C\": [0.1, 1, 10], \"gamma\": ['scale', 'auto']},\n",
    "    \"KNN\": {\"n_neighbors\": list(range(5, 11)), \"weights\": ['uniform', 'distance']},\n",
    "    \"XGBoost\": {\"n_estimators\": [100, 200, 1000], \"learning_rate\": [0.01, 0.1, 0.5], \"max_depth\": [3, 5]}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.971698</td>\n",
       "      <td>0.953704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 10}</td>\n",
       "      <td>0.941520</td>\n",
       "      <td>0.971154</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>{'n_estimators': 100, 'max_depth': None, 'min_...</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.990654</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>{'n_estimators': 200, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC</td>\n",
       "      <td>{'kernel': 'rbf', 'C': 10, 'gamma': 'scale'}</td>\n",
       "      <td>0.953216</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.981481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>{'n_neighbors': 9, 'weights': 'uniform'}</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.963303</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'n_estimators': 200, 'learning_rate': 0.1, 'm...</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>0.990741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model                                    Hyperparameters  \\\n",
       "0  Logistic Regression                                         {'C': 0.1}   \n",
       "1        Decision Tree          {'max_depth': 5, 'min_samples_split': 10}   \n",
       "2        Random Forest  {'n_estimators': 100, 'max_depth': None, 'min_...   \n",
       "3    Gradient Boosting  {'n_estimators': 200, 'learning_rate': 0.1, 'm...   \n",
       "4                  SVC       {'kernel': 'rbf', 'C': 10, 'gamma': 'scale'}   \n",
       "5                  KNN           {'n_neighbors': 9, 'weights': 'uniform'}   \n",
       "6              XGBoost  {'n_estimators': 200, 'learning_rate': 0.1, 'm...   \n",
       "\n",
       "   Accuracy  Precision    Recall  \n",
       "0  0.953216   0.971698  0.953704  \n",
       "1  0.941520   0.971154  0.935185  \n",
       "2  0.982456   0.990654  0.981481  \n",
       "3  0.988304   0.981818  1.000000  \n",
       "4  0.953216   0.946429  0.981481  \n",
       "5  0.959064   0.963303  0.972222  \n",
       "6  0.982456   0.981651  0.990741  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#final results stored here\n",
    "best_results = {}\n",
    "\n",
    "# Train and evaluate models with variations\n",
    "for name, params_dict in models_grid.items():\n",
    "    no_of_params = len(params_dict)\n",
    "    # Get keys and values from param_grid_rf\n",
    "    param_keys = list(params_dict.keys())\n",
    "    param_values = list(params_dict.values())\n",
    "\n",
    "    # Generate all combinations of hyperparameters\n",
    "    import itertools\n",
    "    param_combinations = list(itertools.product(*param_values))\n",
    "\n",
    "    # Initialize best accuracy, precision, and recall for the model\n",
    "    best_accuracy = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "\n",
    "    # Initialize best hyperparameters for the model\n",
    "    best_hyperparameters = None\n",
    "\n",
    "    # Train and evaluate models with variations\n",
    "    for params in param_combinations:\n",
    "        # Set hyperparameters\n",
    "        hyperparameters = {param_keys[i]: params[i] for i in range(len(param_keys))}\n",
    "        models[name].set_params(**hyperparameters)\n",
    "        # Train the model\n",
    "        models[name].fit(X_train, Y_train)\n",
    "        # Make predictions\n",
    "        pred = models[name].predict(X_test)\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy = accuracy_score(Y_test, pred)\n",
    "        precision = precision_score(Y_test, pred)\n",
    "        recall = recall_score(Y_test, pred)\n",
    "        # Check if current metrics are better than the previous best\n",
    "        if accuracy > best_accuracy:\n",
    "            # Update best metrics and hyperparameters\n",
    "            best_accuracy = accuracy\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "            best_hyperparameters = hyperparameters\n",
    "\n",
    "    # Store the best results for the model\n",
    "    best_results[name] = {'Model': name, \n",
    "                          'Hyperparameters': best_hyperparameters, \n",
    "                          'Accuracy': best_accuracy,\n",
    "                          'Precision': best_precision,\n",
    "                          'Recall': best_recall}\n",
    "\n",
    "# Create DataFrame from the best results dictionary\n",
    "best_results_df = pd.DataFrame(list(best_results.values()))\n",
    "\n",
    "# Display\n",
    "display(best_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extra Testing and Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score \n",
    "\n",
    "def loss(y_true, y_pred, retur=False): \n",
    "    pre = precision_score(y_true, y_pred, average='macro')  \n",
    "    rec = recall_score(y_true, y_pred, average='macro') \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    if retur:\n",
    "        return pre, rec, accuracy \n",
    "    else: \n",
    "        print(pre) \n",
    "        print(rec) \n",
    "        print(accuracy)\n",
    "def train_and_eval(models, X_train, X_test, Y_train, Y_test ): \n",
    "    for name, model in models.items(): \n",
    "        print(name ,':')  \n",
    "        model.fit(X_train, Y_train )\n",
    "        loss(Y_test, model.predict(X_test)) \n",
    "        print('*' * 30) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.8830\n",
      "Logistic_Regression :\n",
      "0.9443734015345269\n",
      "0.9596560846560847\n",
      "0.9532163742690059\n",
      "******************************\n",
      "SVC :\n",
      "0.3157894736842105\n",
      "0.5\n",
      "0.631578947368421\n",
      "******************************\n",
      "DecisionTreeClassifier :\n",
      "0.8985109717868338\n",
      "0.8736772486772486\n",
      "0.8947368421052632\n",
      "******************************\n",
      "GradientBoostingClassifier :\n",
      "0.9225689404934688\n",
      "0.9279100529100529\n",
      "0.9298245614035088\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = test_logistic_regression(X_test, Y_test, theta)\n",
    "print(f\"Logistic regression model's accuracy = {accuracy:.4f}\")\n",
    "train_and_eval(models, X_train, X_test, Y_train, Y_test )\n",
    "#X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "Youtube:-  https://www.youtube.com/c/neweraa \n",
    "\n",
    "Thanks "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
